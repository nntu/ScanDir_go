# PHÃ‚N TÃCH LOGIC DUPLICATION DETECTION - PHASE 2

## ğŸ“‹ Tá»”NG QUAN QUY TRÃŒNH

Phase 2 thá»±c hiá»‡n viá»‡c phÃ¡t hiá»‡n file trÃ¹ng láº·p theo 5 bÆ°á»›c chÃ­nh:

### BÆ¯á»šC 1: XÃC Äá»ŠNH NGHI NGá»œ LÃ€ DUPLICATE (Dá»±a trÃªn SIZE)

**Query SQL:**
```sql
SELECT size
FROM fs_files
WHERE size > 0 AND hash_value IS NULL
GROUP BY size
HAVING COUNT(*) > 1  -- Chá»‰ láº¥y cÃ¡c size cÃ³ >= 2 files
```

**Logic:**
- Chá»‰ xá»­ lÃ½ cÃ¡c file cÃ³ `size > 0` vÃ  chÆ°a cÃ³ `hash_value`
- NhÃ³m theo `size` vÃ  chá»‰ láº¥y cÃ¡c nhÃ³m cÃ³ >= 2 files
- **LÃ½ do:** File cÃ¹ng size cÃ³ kháº£ nÄƒng cao lÃ  duplicate (nhÆ°ng chÆ°a cháº¯c cháº¯n)

**VÃ­ dá»¥:**
```
File A: size = 1024 bytes, hash = NULL
File B: size = 1024 bytes, hash = NULL
File C: size = 2048 bytes, hash = NULL
â†’ Chá»‰ File A vÃ  B Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u nghi ngá» (cÃ¹ng size 1024)
â†’ File C bá»‹ bá» qua (size duy nháº¥t)
```

---

### BÆ¯á»šC 2: Láº¤Y TOÃ€N Bá»˜ FILE CÃ™NG SIZE

**Query SQL:**
```sql
SELECT f1.id, f1.path
FROM fs_files f1
INNER JOIN (
    SELECT size FROM fs_files
    WHERE size > 0 AND hash_value IS NULL
    GROUP BY size HAVING COUNT(*) > 1
) f2 ON f1.size = f2.size
WHERE f1.size > 0 AND f1.hash_value IS NULL
ORDER BY f1.size
```

**Logic:**
- Láº¥y táº¥t cáº£ file cÃ³ cÃ¹ng size vá»›i cÃ¡c nhÃ³m nghi ngá»
- Chá»‰ láº¥y file chÆ°a cÃ³ hash (`hash_value IS NULL`)
- Sáº¯p xáº¿p theo size Ä‘á»ƒ xá»­ lÃ½ theo nhÃ³m

**Káº¿t quáº£:**
- Danh sÃ¡ch `FileToHash[]` chá»©a `{ID, Path}` cá»§a táº¥t cáº£ file nghi ngá»

---

### BÆ¯á»šC 3: TÃNH HASH CHO CÃC FILE NGHI NGá»œ

**Worker Pool:**
```go
for w := 0; w < cfg.MaxWorkers; w++ {
    go func() {
        for job := range jobs {
            hash, err := calculateHashWithContext(ctx, job.Path)
            results <- HashResult{ID: job.ID, Hash: hash, Err: err}
        }
    }()
}
```

**Logic:**
- Sá»­ dá»¥ng worker pool Ä‘á»ƒ tÃ­nh hash song song
- Má»—i worker Ä‘á»c file vÃ  tÃ­nh MD5 hash
- Káº¿t quáº£ Ä‘Æ°á»£c gá»­i vÃ o channel `results`

**Hash Algorithm:** MD5
- Äá»c file theo chunks 64KB
- Há»— trá»£ timeout vÃ  context cancellation
- Bá» qua file rá»—ng (size = 0)

---

### BÆ¯á»šC 4: UPDATE HASH VÃ€O DATABASE

**Update Statement:**
```sql
UPDATE fs_files SET hash_value = ? WHERE id = ?
```

**Logic:**
- Batch processing: Commit má»—i 1000 records
- Sá»­ dá»¥ng transaction Ä‘á»ƒ Ä‘áº£m báº£o consistency
- Chá»‰ update file cÃ³ hash há»£p lá»‡ (`hash.Valid == true`)

**VÃ­ dá»¥:**
```
File A (ID=1): hash = "abc123" â†’ UPDATE thÃ nh cÃ´ng
File B (ID=2): hash = "abc123" â†’ UPDATE thÃ nh cÃ´ng
File C (ID=3): hash = "def456" â†’ UPDATE thÃ nh cÃ´ng
```

---

### BÆ¯á»šC 5: ÄÃNH Dáº¤U FILE TRÃ™NG (Hiá»‡n táº¡i CHÆ¯A cÃ³)

**âš ï¸ Váº¤N Äá»€ HIá»†N Táº I:**
- Code chá»‰ update `hash_value` vÃ o DB
- **KHÃ”NG cÃ³ bÆ°á»›c Ä‘Ã¡nh dáº¥u duplicate ngay sau khi tÃ­nh hash**
- Viá»‡c xÃ¡c Ä‘á»‹nh duplicate chá»‰ Ä‘Æ°á»£c thá»±c hiá»‡n á»Ÿ pháº§n **REPORT** khi query láº¡i

**Report Query (sau nÃ y):**
```sql
SELECT hash_value
FROM fs_files
WHERE hash_value IS NOT NULL
GROUP BY hash_value
HAVING COUNT(*) > 1  -- TÃ¬m hash cÃ³ >= 2 files
```

---

## ğŸ” PHÃ‚N TÃCH CHI TIáº¾T

### Flow Diagram:

```
PHASE 1: SCAN METADATA
    â†“
[fs_files table]
- id, path, size, hash_value=NULL
    â†“
PHASE 2: DUPLICATION DETECTION
    â†“
Step 1: TÃ¬m size cÃ³ >= 2 files
    â†“
Step 2: Láº¥y táº¥t cáº£ file cÃ¹ng size
    â†“
Step 3: TÃ­nh hash (MD5) song song
    â†“
Step 4: Update hash_value vÃ o DB
    â†“
[fs_files table]
- id, path, size, hash_value="abc123"
    â†“
REPORT PHASE (sau nÃ y)
    â†“
Query: GROUP BY hash_value HAVING COUNT > 1
    â†“
[Káº¿t quáº£: Danh sÃ¡ch duplicate groups]
```

---

## âš ï¸ Váº¤N Äá»€ VÃ€ Háº N CHáº¾

### 1. **KhÃ´ng Ä‘Ã¡nh dáº¥u duplicate ngay láº­p tá»©c**
- Pháº£i query láº¡i á»Ÿ report phase
- KhÃ´ng cÃ³ flag `is_duplicate` trong database
- KhÃ´ng biáº¿t file nÃ o lÃ  duplicate cho Ä‘áº¿n khi report

### 2. **Xá»­ lÃ½ file size duy nháº¥t**
- File cÃ³ size duy nháº¥t khÃ´ng Ä‘Æ°á»£c tÃ­nh hash
- CÃ³ thá»ƒ bá» sÃ³t duplicate náº¿u:
  - File bá»‹ xÃ³a sau khi scan
  - File Ä‘Æ°á»£c thÃªm vÃ o sau khi scan
  - File cÃ³ size khÃ¡c nhau nhÆ°ng ná»™i dung giá»‘ng (ráº¥t hiáº¿m)

### 3. **KhÃ´ng cÃ³ thá»‘ng kÃª real-time**
- KhÃ´ng biáº¿t cÃ³ bao nhiÃªu duplicate groups
- KhÃ´ng biáº¿t tá»•ng dung lÆ°á»£ng duplicate
- Pháº£i chá» Ä‘áº¿n report phase

---

## ğŸ’¡ Äá»€ XUáº¤T Cáº¢I TIáº¾N

### Option 1: ThÃªm cá»™t `is_duplicate` vÃ o database

**Schema:**
```sql
ALTER TABLE fs_files ADD COLUMN is_duplicate BOOLEAN DEFAULT 0;
CREATE INDEX idx_file_duplicate ON fs_files(is_duplicate) WHERE is_duplicate = 1;
```

**Logic sau khi tÃ­nh hash:**
```go
// Sau khi update hash, kiá»ƒm tra vÃ  Ä‘Ã¡nh dáº¥u duplicate
UPDATE fs_files 
SET is_duplicate = 1 
WHERE hash_value IN (
    SELECT hash_value 
    FROM fs_files 
    WHERE hash_value IS NOT NULL 
    GROUP BY hash_value 
    HAVING COUNT(*) > 1
)
```

### Option 2: Táº¡o báº£ng `duplicate_groups` riÃªng

**Schema:**
```sql
CREATE TABLE duplicate_groups (
    hash_value TEXT PRIMARY KEY,
    file_count INTEGER,
    total_size BIGINT,
    first_seen DATETIME
);

CREATE TABLE duplicate_files (
    file_id INTEGER,
    hash_value TEXT,
    FOREIGN KEY (file_id) REFERENCES fs_files(id),
    FOREIGN KEY (hash_value) REFERENCES duplicate_groups(hash_value)
);
```

**Logic:**
- Sau khi tÃ­nh hash xong, insert vÃ o `duplicate_groups`
- Link file vá»›i group qua `duplicate_files`
- Dá»… query vÃ  thá»‘ng kÃª hÆ¡n

### Option 3: ÄÃ¡nh dáº¥u ngay trong quÃ¡ trÃ¬nh update (Recommended)

**Cáº£i tiáº¿n `commitHashBatch`:**
```go
func commitHashBatch(ctx context.Context, db *sql.DB, batch []HashResult, logger *ScannerLogger) int {
    // 1. Update hash_value
    // 2. Sau khi commit, kiá»ƒm tra vÃ  Ä‘Ã¡nh dáº¥u duplicate ngay
    // 3. Sá»­ dá»¥ng má»™t query Ä‘á»ƒ update táº¥t cáº£ duplicate cÃ¹ng lÃºc
}
```

**Query Ä‘Ã¡nh dáº¥u duplicate:**
```sql
UPDATE fs_files 
SET is_duplicate = 1 
WHERE hash_value IN (
    SELECT hash_value 
    FROM fs_files 
    WHERE hash_value IN (?, ?, ...)  -- CÃ¡c hash vá»«a update
    GROUP BY hash_value 
    HAVING COUNT(*) > 1
)
```

---

## ğŸ“Š THá»NG KÃŠ VÃ€ MONITORING

### Metrics nÃªn track:
1. **Sá»‘ file nghi ngá»:** Tá»•ng file cÃ¹ng size
2. **Sá»‘ file Ä‘Ã£ hash:** File Ä‘Ã£ tÃ­nh hash thÃ nh cÃ´ng
3. **Sá»‘ duplicate groups:** NhÃ³m file cÃ³ cÃ¹ng hash
4. **Tá»•ng dung lÆ°á»£ng duplicate:** Tá»•ng size cá»§a duplicate files
5. **Thá»i gian hash:** Thá»i gian tÃ­nh hash trung bÃ¬nh

### Logging nÃªn cÃ³:
- Sá»‘ file nghi ngá» ban Ä‘áº§u
- Tiáº¿n Ä‘á»™ hash (má»—i 1000 files)
- Sá»‘ duplicate groups phÃ¡t hiá»‡n Ä‘Æ°á»£c
- Tá»•ng dung lÆ°á»£ng duplicate

---

## ğŸ¯ Káº¾T LUáº¬N

**Logic hiá»‡n táº¡i:**
âœ… Tá»‘i Æ°u vá» performance (loáº¡i bá» N+1 queries)
âœ… Batch processing hiá»‡u quáº£
âœ… Worker pool song song
âŒ Thiáº¿u bÆ°á»›c Ä‘Ã¡nh dáº¥u duplicate ngay láº­p tá»©c
âŒ KhÃ´ng cÃ³ thá»‘ng kÃª real-time

**Khuyáº¿n nghá»‹:**
1. ThÃªm cá»™t `is_duplicate` vÃ o database
2. ÄÃ¡nh dáº¥u duplicate ngay sau khi update hash
3. ThÃªm thá»‘ng kÃª vÃ  logging chi tiáº¿t hÆ¡n
4. CÃ³ thá»ƒ táº¡o báº£ng `duplicate_groups` Ä‘á»ƒ query nhanh hÆ¡n

